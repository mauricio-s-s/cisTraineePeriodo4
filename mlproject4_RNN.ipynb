{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tQhoktfHKC3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = os.path.join(os.getcwd(),'Musical_instruments_reviews.csv')\n",
        "df = pd.read_csv(datasetPath)"
      ],
      "metadata": {
        "id": "XDSNkf70m6dh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xnp = np.array(df['reviewText'])\n",
        "ynp = np.array(df['overall'])"
      ],
      "metadata": {
        "id": "OrKtE8qEm_vm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = df['reviewText'][0]\n",
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9yO4d0_wocwe",
        "outputId": "09cb5271-278d-458b-fbe6-2291c378775c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#certificar-se de que todas as instancias sejam strings\n",
        "for i in range(len(xnp)):\n",
        "    if type(xnp[i]) != str:\n",
        "        xnp[i] = str(xnp[i])\n",
        "\n",
        "#serÃ£o considerados positivos overall > 2\n",
        "ynp = (ynp > 3).astype(int)"
      ],
      "metadata": {
        "id": "hZFfFIKCoU2I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separar train_set, valid_test e  test_set\n",
        "X_train, X_test, y_train, y_test = train_test_split(xnp, ynp, test_size=0.2, random_state=42, stratify=ynp)"
      ],
      "metadata": {
        "id": "WcL1K60coWbv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X_batch,y_batch):\n",
        "    #X_batch = tf.strings.substr(X_batch, 0, 100)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"),y_batch"
      ],
      "metadata": {
        "id": "b2yqimZYopmW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "n_batches = len(X_train)//batch_size + 1"
      ],
      "metadata": {
        "id": "BZ5NG_7So0GO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dataset, this will return object of TensorSliceDataset\n",
        "trainDataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train),\n",
        "                                                  tf.convert_to_tensor(y_train)))\n",
        "trainDataset = trainDataset.batch(batch_size=batch_size).map(preprocess)"
      ],
      "metadata": {
        "id": "6rgYjetmp_2e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construct the vocabulary (Bag of words method)\n",
        "vocabulary = Counter()"
      ],
      "metadata": {
        "id": "mYzILRNuqQn_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for X_batch,y_batch in trainDataset:\n",
        "    for textPiece in X_batch:\n",
        "        vocabulary.update(list(textPiece.numpy()))"
      ],
      "metadata": {
        "id": "upCXpaGGqAi2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n"
      ],
      "metadata": {
        "id": "f0BP6O7zo6P9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we need to add a preprocessing step to replace each word with its ID (i.e., its\n",
        "#index in the vocabulary). Just like we did in Chapter 13, we will create a lookup table\n",
        "#for this, using 1,000 out-of-vocabulary (oov) buckets:\n",
        "\n",
        "\n",
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ],
      "metadata": {
        "id": "AkJ-7wqhqi7P"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n"
      ],
      "metadata": {
        "id": "0_VNTjsDql4-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = trainDataset.map(encode_words).prefetch(1)"
      ],
      "metadata": {
        "id": "qMlhksjNqpI2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(trainDataset, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpLwoZhQqqdw",
        "outputId": "25e0a99d-e14a-4afc-b7fe-9078bcc20560"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "83/83 [==============================] - 195s 2s/step - loss: 0.0491 - accuracy: 0.9879\n",
            "Epoch 2/5\n",
            "83/83 [==============================] - 188s 2s/step - loss: 2.9432e-06 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "83/83 [==============================] - 190s 2s/step - loss: 2.8773e-06 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "83/83 [==============================] - 196s 2s/step - loss: 2.8018e-06 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "83/83 [==============================] - 186s 2s/step - loss: 2.7217e-06 - accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ]
}